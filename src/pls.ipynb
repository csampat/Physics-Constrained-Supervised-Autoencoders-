{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd0cef7cd0469155a29afd544290d573430e35716703753df15e7b5f11a91b75929",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e48751a2e34d595a2cec059cf3591fb26f52f348ef6ba549e15b56ca6a208700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cwd = os.getcwd()\n",
    "fileLoc = 'dataFiles/pls_200points_MRT_torqueAdded.csv'\n",
    "# fileLoc = 'dataFiles/test_data1_WithOutputs_withCalculatedValues.csv'\n",
    "dataFile_original = pd.read_csv(os.path.join(cwd,fileLoc))\n",
    "# print(dataFile_original.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile_incomplete = dataFile_original.drop(['Year','Experiments', 'Sr No','Screw Configuration','Liq add position','DetMRT','RanTorque','DetTorque','RanMRT','Regime'],axis=1)\n",
    "# dataFile_incomplete = dataFile_original.drop(['Experiments', 'Sr No','Screw Configuration','Liq add position','DetMRT','RanTorque','DetTorque','RanMRT','Regime','Beta','Exp Fill level','Fines %'],axis=1)\n",
    "# print(dataFile_incomplete.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"None of [Index(['DetTorque', 'DetMRT'], dtype='object')] are in the [columns]\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6d48213397c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# ranMRT = dataFile_incomplete['RanMRT']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataFile_DettorqueMRT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdcm_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineaeRegressionModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Torque'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'MRT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdataFile_incomplete\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DetTorque'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DetMRT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataFile_DettorqueMRT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdataFile_original\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DetTorque'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DetMRT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataFile_DettorqueMRT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdataFile_RantorqueMRT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdcm_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomImputationRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Torque'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'MRT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3new\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3482\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3483\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3484\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3486\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3new\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3509\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3510\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3511\u001b[1;33m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3512\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3513\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3new\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3new\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3new\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 raise KeyError(\n\u001b[0;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1177\u001b[1;33m                         \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m                     )\n\u001b[0;32m   1179\u001b[0m                 )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['DetTorque', 'DetMRT'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from HelperTools import DataCompletionMethods\n",
    "dcm_object = DataCompletionMethods.DataCompletionMethods(dataFile_incomplete)\n",
    "# detMRT = dataFile_incomplete['DetMRT']\n",
    "# ranMRT = dataFile_incomplete['RanMRT']\n",
    "dataFile_DettorqueMRT = dcm_object.lineaeRegressionModel(['Torque','MRT'],True)\n",
    "dataFile_incomplete[['Det_Torque','Det_MRT']] = dataFile_DettorqueMRT\n",
    "dataFile_original[['Det_Torque','Det_MRT']] = dataFile_DettorqueMRT\n",
    "dataFile_RantorqueMRT = dcm_object.randomImputationRegression(['Torque','MRT'],True)\n",
    "dataFile_incomplete[['Ran_Torque','Ran_MRT']] = dataFile_RantorqueMRT\n",
    "dataFile_original[['Ran_Torque','Ran_MRT']] = dataFile_RantorqueMRT\n",
    "ranMRT = dataFile_incomplete['Ran_MRT']\n",
    "FillLevel = dcm_object.fillLevel_osorio(ranMRT)\n",
    "dataFile_complete = dataFile_incomplete\n",
    "# dataFile_complete.to_csv('newDataCompleted.csv')\n",
    "# dataFile_complete[\"Calc fill level\"] = FillLevel\n",
    "# print(dataFile_complete[\"Calc fill level\"].describe())\n",
    "# FillLevel_lalith = dcm_object.fillevel_lalith(detMRT)\n",
    "# dataFile_complete[\"Lal Calc fill level\"] = FillLevel_lalith\n",
    "# print(dataFile_complete[\"Lal Calc fill level\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition  import PLSRegression, PLSCanonical\n",
    "dataFile_complete = dataFile_complete.drop(['Torque','MRT'],axis=1)\n",
    "train_dataset = dataFile_complete.sample(frac=0.8,random_state=42)\n",
    "exp_traindataset = dataFile_original['Experiments'].sample(frac=0.8,random_state=42)\n",
    "test_dataset = dataFile_complete.drop(train_dataset.index)\n",
    "exp_testdataset = dataFile_original['Experiments'].drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['DetTorque','RanMRT','final d50']\n",
    "train_labels = pd.DataFrame([train_dataset.pop(i) for i in labels]).T\n",
    "test_labels = pd.DataFrame([test_dataset.pop(i) for i in labels]).T\n",
    "print(train_labels.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls1 = PLSRegression(n_components=2,scale=True)\n",
    "pls1.fit(train_dataset,train_labels)\n",
    "print(np.round(pls1.coef_,3))\n",
    "pls1.predict(test_dataset)\n",
    "pls1.score(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = pls1.transform(train_dataset)\n",
    "extent = np.divide(train_labels['final d50'],train_dataset['Initial d50'])\n",
    "exps = exp_traindataset\n",
    "fig,ax = plt.subplots(1,2,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(ad[i,0],ad[i,1],label=g)\n",
    "ax[0].set_xlabel('LV 1')\n",
    "ax[0].set_ylabel('LV 2')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "train_dataset_pls = train_dataset.copy()\n",
    "train_dataset_pls['Extent of gran'] = extent\n",
    "train_dataset_pls['LV 1'] = ad[:,0]\n",
    "train_dataset_pls['LV 2'] = ad[:,1]\n",
    "groups = train_dataset_pls.groupby(pd.cut(train_dataset_pls['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[1].set_xlabel('LV 1')\n",
    "ax[1].set_ylabel('LV 2')\n",
    "ax[1].set_xlim([-5,5])\n",
    "ax[1].set_ylim([-5,5])\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(10,5)\n",
    "fig.suptitle('PLS latent variables',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0)"
   ]
  },
  {
   "source": [
    "# Tsne \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "train_dataset_minmax = preprocessing.MinMaxScaler().fit_transform(train_dataset)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "train_dataset_transform = tsne.fit_transform(train_dataset_minmax)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(train_dataset_transform[i,0],train_dataset_transform[i,1],label=g)\n",
    "ax[0].set_xlabel('LV 1')\n",
    "ax[0].set_ylabel('LV 2')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "train_dataset_tsne = train_dataset.copy()\n",
    "train_dataset_tsne['Extent of gran'] = extent\n",
    "train_dataset_tsne['LV 1'] = train_dataset_transform[:,0]\n",
    "train_dataset_tsne['LV 2'] = train_dataset_transform[:,1]\n",
    "groups = train_dataset_tsne.groupby(pd.cut(train_dataset_tsne['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[1].set_xlabel('LV 1')\n",
    "ax[1].set_ylabel('LV 2')\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(10,5)\n",
    "fig.suptitle('TSNE latent variables',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "prinComps = pca.fit_transform(train_dataset_minmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(prinComps[i,0],prinComps[i,1],label=g)\n",
    "ax[0].set_xlabel('LV 1')\n",
    "ax[0].set_ylabel('LV 2')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "train_dataset_pca = train_dataset.copy()\n",
    "train_dataset_pca['Extent of gran'] = extent\n",
    "train_dataset_pca['LV 1'] = prinComps[:,0]\n",
    "train_dataset_pca['LV 2'] = prinComps[:,1]\n",
    "groups = train_dataset_pca.groupby(pd.cut(train_dataset_pca['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[1].set_xlabel('LV 1')\n",
    "ax[1].set_ylabel('LV 2')\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(10,5)\n",
    "fig.suptitle('PCA latent variables',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# autoencoder\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "import seaborn as sns\n",
    "\n",
    "#### linear encoder\n",
    "#input layer\n",
    "\n",
    "input_layer = Input(shape=(train_dataset_minmax.shape[1],))\n",
    "\n",
    "# encoding using only 1 linear layer\n",
    "encoder1 = Dense(2,activation='linear')(input_layer)\n",
    "\n",
    "# decoder 0 layers and output \n",
    "output1 = Dense(train_dataset_minmax.shape[1],activation='linear')(encoder1)\n",
    "\n",
    "autoencoder = Model(input_layer,output1)\n",
    "autoencoder.compile(optimizer='adadelta',loss='mse')\n",
    "print(autoencoder.summary())\n",
    "autoencoder.fit(train_dataset_minmax,train_dataset_minmax,epochs=1,shuffle=False,validation_split=0.25,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting latent variable values \n",
    "hidden_representation = Sequential()\n",
    "hidden_representation.add(autoencoder.layers[0])\n",
    "hidden_representation.add(autoencoder.layers[1])\n",
    "\n",
    "latent_rep = np.array(hidden_representation.predict(train_dataset_minmax))\n",
    "fig,ax = plt.subplots(1,2,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(latent_rep[i,0],latent_rep[i,1],label=g)\n",
    "ax[0].set_xlabel('LV 1')\n",
    "ax[0].set_ylabel('LV 2')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "train_dataset_aeliner = train_dataset.copy()\n",
    "train_dataset_aeliner['Extent of gran'] = extent\n",
    "train_dataset_aeliner['LV 1'] = latent_rep[:,0]\n",
    "train_dataset_aeliner['LV 2'] = latent_rep[:,1]\n",
    "groups = train_dataset_aeliner.groupby(pd.cut(train_dataset_aeliner['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[1].set_xlabel('LV 1')\n",
    "ax[1].set_ylabel('LV 2')\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(10,5)\n",
    "fig.suptitle('Linear AE latent variables',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### linear 2 layer encoder\n",
    "#input layer\n",
    "\n",
    "input_layer = Input(shape=(train_dataset_minmax.shape[1],))\n",
    "\n",
    "# encoding using only 1 linear layer\n",
    "encoder2 = Dense(4,activation='linear')(input_layer)\n",
    "encoder3 = Dense(2,activation='linear')(encoder2)\n",
    "\n",
    "# decoder 0 layers and output \n",
    "decoder1 = Dense(4,activation='linear')(encoder3)\n",
    "output2 = Dense(train_dataset_minmax.shape[1],activation='linear')(decoder1)\n",
    "\n",
    "autoencoder2 = Model(input_layer,output2)\n",
    "autoencoder2.compile(optimizer='adadelta',loss='mse')\n",
    "print(autoencoder2.summary())\n",
    "autoencoder2.fit(train_dataset_minmax,train_dataset_minmax,epochs=1,shuffle=True,validation_split=0.25)\n",
    "#getting latent variable values \n",
    "hidden_representation2 = Sequential()\n",
    "hidden_representation2.add(autoencoder2.layers[0])\n",
    "hidden_representation2.add(autoencoder2.layers[1])\n",
    "hidden_representation2.add(autoencoder2.layers[2])\n",
    "\n",
    "latent_rep2 = np.array(hidden_representation2.predict(train_dataset_minmax))\n",
    "fig,ax = plt.subplots(1,2,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(latent_rep2[i,0],latent_rep2[i,1],label=g)\n",
    "ax[0].set_xlabel('LV 1')\n",
    "ax[0].set_ylabel('LV 2')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "train_dataset_aeliner2 = train_dataset.copy()\n",
    "train_dataset_aeliner2['Extent of gran'] = extent\n",
    "train_dataset_aeliner2['LV 1'] = latent_rep2[:,0]\n",
    "train_dataset_aeliner2['LV 2'] = latent_rep2[:,1]\n",
    "groups = train_dataset_aeliner2.groupby(pd.cut(train_dataset_aeliner2['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[1].set_xlabel('LV 1')\n",
    "ax[1].set_ylabel('LV 2')\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(10,5)\n",
    "fig.suptitle('Linear AE 2 latent variables',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### non-linear 1 layer encoder\n",
    "#input layer\n",
    "\n",
    "input_layer = Input(shape=(train_dataset_minmax.shape[1],))\n",
    "\n",
    "# encoding using only 1 linear layer\n",
    "encoder4 = Dense(3,activation='tanh')(input_layer)\n",
    "\n",
    "\n",
    "# decoder 0 layers and output \n",
    "\n",
    "output3 = Dense(train_dataset_minmax.shape[1],activation='tanh')(encoder4)\n",
    "\n",
    "autoencoder3 = Model(input_layer,output3)\n",
    "autoencoder3.compile(optimizer='adadelta',loss='mse')\n",
    "print(autoencoder3.summary())\n",
    "autoencoder3.fit(train_dataset_minmax,train_dataset_minmax,epochs=1,shuffle=True,validation_split=0.25)\n",
    "#getting latent variable values \n",
    "hidden_representation3 = Sequential()\n",
    "hidden_representation3.add(autoencoder3.layers[0])\n",
    "hidden_representation3.add(autoencoder3.layers[1])\n",
    "\n",
    "\n",
    "latent_rep3 = np.array(hidden_representation3.predict(train_dataset_minmax))\n",
    "fig,ax = plt.subplots(1,2,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(latent_rep3[i,0],latent_rep3[i,1],label=g)\n",
    "ax[0].set_xlabel('LV 1')\n",
    "ax[0].set_ylabel('LV 2')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "train_dataset_aenonlin1 = train_dataset.copy()\n",
    "train_dataset_aenonlin1['Extent of gran'] = extent\n",
    "train_dataset_aenonlin1['LV 1'] = latent_rep3[:,0]\n",
    "train_dataset_aenonlin1['LV 2'] = latent_rep3[:,1]\n",
    "groups = train_dataset_aenonlin1.groupby(pd.cut(train_dataset_aenonlin1['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[1].set_xlabel('LV 1')\n",
    "ax[1].set_ylabel('LV 2')\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(10,5)\n",
    "fig.suptitle('non-Linear AE 1 latent variables',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### non-linear 2 layer encoder\n",
    "#input layer\n",
    "\n",
    "input_layer = Input(shape=(train_dataset_minmax.shape[1],))\n",
    "\n",
    "# encoding using only 1 linear layer\n",
    "encoder5 = Dense(3,activation='tanh')(input_layer)\n",
    "encoder6 = Dense(2,activation='linear')(encoder5)\n",
    "\n",
    "# decoder 0 layers and output \n",
    "decoder2 = Dense(3,activation='tanh')(encoder6)\n",
    "output4 = Dense(train_dataset_minmax.shape[1],activation='tanh')(decoder2)\n",
    "\n",
    "autoencoder4 = Model(input_layer,output4)\n",
    "autoencoder4.compile(optimizer='adadelta',loss='mse')\n",
    "print(autoencoder4.summary())\n",
    "autoencoder4.fit(train_dataset_minmax,train_dataset_minmax,epochs=1,shuffle=True,validation_split=0.25,verbose=0)\n",
    "#getting latent variable values \n",
    "hidden_representation4 = Sequential()\n",
    "hidden_representation4.add(autoencoder4.layers[0])\n",
    "hidden_representation4.add(autoencoder4.layers[1])\n",
    "hidden_representation4.add(autoencoder4.layers[2])\n",
    "\n",
    "latent_rep4 = np.array(hidden_representation4.predict(train_dataset_minmax))\n",
    "fig,ax = plt.subplots(1,3,sharey=True,subplot_kw=dict(box_aspect=1.25))\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(latent_rep4[i,0],latent_rep4[i,1],label=g)\n",
    "ax[0].set_xlabel('LV 1')\n",
    "ax[0].set_ylabel('LV 2')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "train_dataset_aenonlin2 = train_dataset.copy()\n",
    "train_dataset_aenonlin2['Extent of gran'] = extent\n",
    "train_dataset_aenonlin2['LV 1'] = latent_rep4[:,0]\n",
    "train_dataset_aenonlin2['LV 2'] = latent_rep4[:,1]\n",
    "groups = train_dataset_aenonlin2.groupby(pd.cut(train_dataset_aenonlin2['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[1].set_xlabel('LV 1')\n",
    "ax[1].set_ylabel('LV 2')\n",
    "ax[2].set_title('Extent of granulation')\n",
    "fig.set_size_inches(15,5)\n",
    "fig.suptitle('non-Linear AE 2 latent variables',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)\n",
    "ranges = [0,0.25,0.5,0.75,1]\n",
    "train_dataset_aenonlin2['Calc Fill level'] = dcm_object.fillLevel_osorio(train_dataset_aenonlin2['DetMRT'])\n",
    "groups = train_dataset_aenonlin2.groupby(pd.cut(train_dataset_aenonlin2['Calc Fill level'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[2].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[2].set_xlabel('LV 1')\n",
    "ax[2].set_ylabel('LV 2')\n",
    "ax[2].set_title('Fill level')\n",
    "ax[2].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from numpy import unique, where\n",
    "\n",
    "kmeans_clus = KMeans(n_clusters=5)\n",
    "kmeans_clus.fit(latent_rep4)\n",
    "y_kmeans = kmeans_clus.predict(latent_rep4)\n",
    "clusters = unique(y_kmeans)\n",
    "print(kmeans_clus.cluster_centers_)\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(y_kmeans == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(latent_rep4[row_ix, 0], latent_rep4[row_ix, 1], label=cluster)\n",
    "plt.legend()\n",
    "plt.xlabel('LV 1')\n",
    "plt.ylabel('LV 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### non-linear 2 layer encoder supervised learning\n",
    "#input layer\n",
    "\n",
    "train_labels['final d50'] = train_labels['final d50'] / 1e6\n",
    "train_labels['DetTorque'] = train_labels['DetTorque'] / 10\n",
    "train_labels['RanMRT'] = train_labels['RanMRT'] / 100\n",
    "\n",
    "input_layer = Input(shape=(train_dataset_minmax.shape[1],))\n",
    "\n",
    "# encoding using only 1 linear layer\n",
    "encoder7 = Dense(3,activation='tanh')(input_layer)\n",
    "encoder8 = Dense(2,activation='linear')(encoder7)\n",
    "\n",
    "# decoder 0 layers and output \n",
    "decoder3 = Dense(3,activation='tanh')(encoder8)\n",
    "output5 = Dense(train_dataset_minmax.shape[1],activation='tanh')(decoder3)\n",
    "\n",
    "decoder_exp1 = Dense(3,activation='tanh')(decoder3)\n",
    "decoder_exp2 = Dense(3,activation='tanh')(decoder_exp1)\n",
    "\n",
    "output_exp1 = Dense(train_labels.shape[1],activation='tanh')(decoder_exp2)\n",
    "autoencoder_sup = Model(input_layer,[output5,output_exp1])\n",
    "autoencoder_sup.compile(optimizer='adadelta',loss='mse')\n",
    "print(autoencoder_sup.summary())\n",
    "autoencoder_sup.fit(train_dataset_minmax,[train_dataset_minmax,train_labels],epochs=100,shuffle=True,validation_split=0.25,verbose=0)\n",
    "#getting latent variable values \n",
    "hidden_representation5 = Sequential()\n",
    "hidden_representation5.add(autoencoder_sup.layers[0])\n",
    "hidden_representation5.add(autoencoder_sup.layers[1])\n",
    "hidden_representation5.add(autoencoder_sup.layers[2])\n",
    "latent_rep5 = np.array(hidden_representation5.predict(train_dataset_minmax))\n",
    "fig,ax = plt.subplots(1,3,sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(latent_rep5[i,0],latent_rep5[i,1],label=g)\n",
    "ax[0].set_xlabel('LV 1')\n",
    "ax[0].set_ylabel('LV 2')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "train_dataset_aenonlin_sup = train_dataset.copy()\n",
    "train_dataset_aenonlin_sup['Extent of gran'] = extent\n",
    "train_dataset_aenonlin_sup['LV 1'] = latent_rep5[:,0]\n",
    "train_dataset_aenonlin_sup['LV 2'] = latent_rep5[:,1]\n",
    "groups = train_dataset_aenonlin_sup.groupby(pd.cut(train_dataset_aenonlin_sup['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[1].set_xlabel('LV 1')\n",
    "ax[1].set_ylabel('LV 2')\n",
    "ax[1].set_title('Extent of granulation')\n",
    "fig.set_size_inches(15,5)\n",
    "fig.suptitle('non-Linear AE 2 latent variables',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)\n",
    "ranges = [0,0.25,0.5,0.75,1]\n",
    "train_dataset_aenonlin_sup['Calc Fill level'] = dcm_object.fillLevel_osorio(train_dataset_aenonlin_sup['DetMRT'])\n",
    "groups = train_dataset_aenonlin_sup.groupby(pd.cut(train_dataset_aenonlin_sup['Calc Fill level'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[2].scatter(group['LV 1'],group['LV 2'],label=val)\n",
    "ax[2].set_xlabel('LV 1')\n",
    "ax[2].set_ylabel('LV 2')\n",
    "ax[2].set_title('Fill level')\n",
    "ax[2].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clus_sup = KMeans(n_clusters=4,algorithm=\"elkan\")\n",
    "kmeans_clus_sup.fit(latent_rep4)\n",
    "y_kmeans_sup = kmeans_clus_sup.predict(latent_rep5)\n",
    "clusters_sup = unique(y_kmeans_sup)\n",
    "sup_center = kmeans_clus_sup.cluster_centers_\n",
    "print(sup_center)\n",
    "for cluster_sup in clusters_sup:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(y_kmeans_sup == cluster_sup)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(latent_rep5[row_ix, 0], latent_rep5[row_ix, 1], label=cluster_sup)\n",
    "plt.legend()\n",
    "plt.scatter(sup_center[:,0],sup_center[:,1],c='black',s=200,alpha=0.5)\n",
    "plt.xlabel('LV 1')\n",
    "plt.ylabel('LV 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "datafile_processparam = dataFile_original[['RPM','L/S Ratio','FlowRate (kg/hr)', 'Temperature']]\n",
    "datafile_material = dataFile_original[['Initial d50','Binder Viscosity (mPa.s)','Flowability (HR)','Bulk Density']]\n",
    "datafile_geometry = dataFile_original[['nCE','Granulator diameter (mm)','L/D Ratio','SA of KE','nKE','Liq add position','nKZ','dKZ']]\n",
    "datafile_allinputs = dataFile_original[['RPM','L/S Ratio','FlowRate (kg/hr)', 'Temperature','Initial d50','Binder Viscosity (mPa.s)','Flowability (HR)','Bulk Density','nCE','Granulator diameter (mm)','L/D Ratio','SA of KE','nKE','Liq add position','nKZ','dKZ']]\n",
    "labels = dataFile_original[['DetTorque','RanMRT','final d50']]\n",
    "\n",
    "datafile_material = preprocessing.MinMaxScaler().fit_transform(datafile_material)\n",
    "datafile_processparam = preprocessing.MinMaxScaler().fit_transform(datafile_processparam)\n",
    "\n",
    "\n",
    "labels['final d50'] = labels['final d50'] / 1e3\n",
    "labels['DetTorque'] = labels['DetTorque'] / 10\n",
    "labels['RanMRT'] = labels['RanMRT'] / 100\n",
    "\n",
    "##LV for Process Parameters\n",
    "input_pp = Input(shape=(datafile_processparam.shape[1],))\n",
    "#hidden layer\n",
    "hidden_pp1 = Dense(2,activation='tanh',name='hid_layer_pp1')(input_pp)\n",
    "# bottleneck layer\n",
    "encoder_pp = Dense(1,activation='tanh',name='lv_layer_pp')(hidden_pp1)\n",
    "\n",
    "\n",
    "##LV for Material Properties\n",
    "input_mat = Input(shape=(datafile_material.shape[1],))\n",
    "#hidden layer\n",
    "hidden_mat = Dense(2,activation='tanh',name='hid_layer_mat')(input_mat)\n",
    "# bottleneck layer\n",
    "encoder_mat = Dense(1,activation='tanh',name='lv_layer_mat')(hidden_mat)\n",
    "\n",
    "## label encoding for liq add position\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "datafile_geometry['Liq add position'] = le.fit_transform(datafile_geometry['Liq add position'])\n",
    "datafile_geometry = preprocessing.MinMaxScaler().fit_transform(datafile_geometry)\n",
    "datafile_allinputs['Liq add position'] = le.fit_transform(datafile_allinputs['Liq add position'])\n",
    "datafile_allinputs_copy = datafile_allinputs.copy()\n",
    "datafile_allinputs = preprocessing.MinMaxScaler().fit_transform(datafile_allinputs)\n",
    "\n",
    "##LV for Geometry\n",
    "input_geo = Input(shape=(datafile_geometry.shape[1],))\n",
    "#hidden layer\n",
    "hidden_geo = Dense(4,activation='tanh',name='hid_layer_geo')(input_geo)\n",
    "# bottleneck layer\n",
    "encoder_geo = Dense(1,activation='tanh',name='lv_layer_geo')(hidden_geo)\n",
    "\n",
    "from keras.layers import Concatenate\n",
    "concat_bottleneck = Concatenate()([encoder_pp,encoder_mat,encoder_geo])\n",
    "# recons decoder layer 1\n",
    "decoder_recons = Dense(6,activation='tanh',name='decoder_layer_recons1')(concat_bottleneck)\n",
    "\n",
    "# output prediction decoder layer 1 \n",
    "\n",
    "decoder_pred = Dense(3,activation='tanh',name='decoder_layer_pred1')(concat_bottleneck)\n",
    "\n",
    "# output layer reconstruction\n",
    "output_recons = Dense(datafile_allinputs.shape[1],activation='tanh',name='recon_out')(decoder_recons)\n",
    "output_pred = Dense(train_labels.shape[1],activation='linear',name='pred_out')(decoder_pred)\n",
    "\n",
    "autoencoder = Model([input_pp,input_mat,input_geo],[output_recons,output_pred])\n",
    "autoencoder.compile(optimizer='Adam',loss='mse',metrics=['mse','mae'])\n",
    "\n",
    "# if fitting is reqd \n",
    "autoencoder.fit([datafile_processparam,datafile_material,datafile_geometry],[datafile_allinputs,labels],epochs=100, shuffle=True,validation_split=0.25,verbose=1)\n",
    "\n",
    "# autoencoder.summary()\n",
    "\n",
    "predictions_y_AE = autoencoder.predict([datafile_processparam,datafile_material,datafile_geometry])\n",
    "\n",
    "# Getting the hidden representation of the \n",
    "hidden_representation_3 = Model([input_pp,input_mat,input_geo],[encoder_pp,encoder_mat,encoder_geo])\n",
    "# hidden_representation_3.add(Concatenate()([autoencoder.layers[0],autoencoder.layers[1],autoencoder.layers[2]]))\n",
    "\n",
    "# hidden_representation_3.summary()\n",
    "latent_rep = np.array(hidden_representation_3.predict([datafile_processparam,datafile_material,datafile_geometry]))\n",
    "# hidden_representation_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_con_y_AE = np.ravel(np.array(predictions_y_AE[1]))\n",
    "pred_con_y_AE = np.reshape(pred_con_y_AE,(len(datafile_processparam),3))\n",
    "con_pre_y_pd = pd.DataFrame({'Pre final d50':pred_con_y_AE[:,2], 'Pre Torque':pred_con_y_AE[:,0], 'Pre MRT':pred_con_y_AE[:,1]})\n",
    "con_pre_y_pd_fin = pd.concat([pd.DataFrame(datafile_allinputs),con_pre_y_pd],axis=1,ignore_index=False)\n",
    "con_pre_y_pd_fin.to_csv('AE_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'qt')\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "# print(latent_rep.shape)\n",
    "len_TrainData = len(train_dataset)\n",
    "# print(len_TrainData)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax.scatter3D(latent_rep[0,i],latent_rep[1,i],latent_rep[2,i],label=g)\n",
    "ax.set_xlabel('PP')\n",
    "ax.set_ylabel('Mat.')\n",
    "ax.set_zlabel('Geo')\n",
    "ax.set_title('According to exp.')\n",
    "ax.legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ax.view_init(45, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "# print(latent_rep.shape)\n",
    "regs = dataFile_original['Regime']\n",
    "# print(len_TrainData)\n",
    "for g in np.unique(regs):\n",
    "    i = np.where(regs==g)\n",
    "    ax.scatter3D(latent_rep[0,i],latent_rep[1,i],latent_rep[2,i],label=g)\n",
    "ax.set_xlabel('PP')\n",
    "ax.set_ylabel('Mat.')\n",
    "ax.set_zlabel('Geo')\n",
    "ax.set_title('According to Regime')\n",
    "ax.legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ax.view_init(-45, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "# print(latent_rep.shape)\n",
    "regs = dataFile_original['Granulator diameter (mm)']\n",
    "# print(len_TrainData)\n",
    "for g in np.unique(regs):\n",
    "    i = np.where(regs==g)\n",
    "    ax.scatter3D(latent_rep[0,i],latent_rep[1,i],latent_rep[2,i],label=g)\n",
    "ax.set_xlabel('PP')\n",
    "ax.set_ylabel('Mat.')\n",
    "ax.set_zlabel('Geo')\n",
    "ax.set_title('According to Granulator size')\n",
    "ax.legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ax.view_init(-45, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "# print(latent_rep.shape)\n",
    "regs = dataFile_original['nKE']\n",
    "# print(len_TrainData)\n",
    "for g in np.unique(regs):\n",
    "    i = np.where(regs==g)\n",
    "    ax.scatter3D(latent_rep[0,i],latent_rep[1,i],latent_rep[2,i],label=g)\n",
    "ax.set_xlabel('PP')\n",
    "ax.set_ylabel('Mat.')\n",
    "ax.set_zlabel('Geo')\n",
    "ax.set_title('According to nKE')\n",
    "ax.legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ax.view_init(-45, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "datafile_allinputs_copy['PP'] = latent_rep[0,:]\n",
    "datafile_allinputs_copy['Mat'] = latent_rep[1,:]\n",
    "datafile_allinputs_copy['geo'] = latent_rep[2,:]\n",
    "fig,ax = plt.subplots(1,3,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(latent_rep[0,i],latent_rep[1,i],label=g)\n",
    "ax[0].set_xlabel('PP')\n",
    "ax[0].set_ylabel('Mat')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "datafile_allinputs_copy['Extent of gran'] = labels['final d50'] / datafile_allinputs_copy['Initial d50'] /1e-6\n",
    "groups = datafile_allinputs_copy.groupby(pd.cut(datafile_allinputs_copy['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['PP'],group['Mat'],label=val)\n",
    "ax[1].set_xlabel('PP')\n",
    "ax[1].set_ylabel('Mat')\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(15,5)\n",
    "fig.suptitle('AE latent representation',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)\n",
    "regs = dataFile_original['Regime']\n",
    "for g in np.unique(regs):\n",
    "    i = np.where(regs==g)\n",
    "    ax[2].scatter(latent_rep[0,i],latent_rep[1,i],label=g)\n",
    "ax[2].set_xlabel('PP')\n",
    "ax[2].set_ylabel('Mat')\n",
    "ax[2].set_title('According to regimes')\n",
    "ax[2].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(1,3,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(latent_rep[1,i],latent_rep[2,i],label=g)\n",
    "ax[0].set_xlabel('Mat')\n",
    "ax[0].set_ylabel('geo')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "datafile_allinputs_copy['Extent of gran'] = labels['final d50'] / datafile_allinputs_copy['Initial d50'] /1e-6\n",
    "groups = datafile_allinputs_copy.groupby(pd.cut(datafile_allinputs_copy['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['Mat'],group['geo'],label=val)\n",
    "ax[1].set_xlabel('Mat')\n",
    "ax[1].set_ylabel('geo')\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(15,5)\n",
    "fig.suptitle('AE latent representation',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)\n",
    "for g in np.unique(regs):\n",
    "    i = np.where(regs==g)\n",
    "    ax[2].scatter(latent_rep[1,i],latent_rep[2,i],label=g)\n",
    "ax[2].set_xlabel('Mat')\n",
    "ax[2].set_ylabel('geo')\n",
    "ax[2].set_title('According to regimes')\n",
    "ax[2].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,subplot_kw=dict(box_aspect=1),\n",
    "    sharey=True)\n",
    "for g in np.unique(exps):\n",
    "    i = np.where(exps==g)\n",
    "    ax[0].scatter(latent_rep[0,i],latent_rep[2,i],label=g)\n",
    "ax[0].set_xlabel('PP')\n",
    "ax[0].set_ylabel('geo')\n",
    "ax[0].set_title('According to exp.')\n",
    "ax[0].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "ranges = [0,15,20,25,30,35,50]\n",
    "datafile_allinputs_copy['Extent of gran'] = labels['final d50'] / datafile_allinputs_copy['Initial d50'] /1e-6\n",
    "groups = datafile_allinputs_copy.groupby(pd.cut(datafile_allinputs_copy['Extent of gran'],ranges))\n",
    "for val, group in groups:\n",
    "    ax[1].scatter(group['PP'],group['geo'],label=val)\n",
    "ax[1].set_xlabel('PP')\n",
    "ax[1].set_ylabel('geo')\n",
    "plt.title('Extent of granulation')\n",
    "fig.set_size_inches(15,5)\n",
    "fig.suptitle('AE latent representation',fontsize=16)\n",
    "ax[1].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0)\n",
    "for g in np.unique(regs):\n",
    "    i = np.where(regs==g)\n",
    "    ax[2].scatter(latent_rep[0,i],latent_rep[2,i],label=g)\n",
    "ax[2].set_xlabel('PP')\n",
    "ax[2].set_ylabel('geo')\n",
    "ax[2].set_title('According to regimes')\n",
    "ax[2].legend(bbox_to_anchor=(0., -0.5, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}